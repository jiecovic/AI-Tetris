# @package _global_
callbacks:
  latest:
    enabled: true                     # save latest checkpoints
    every: 200_000                     # save latest every N env steps
  eval_checkpoint:
    enabled: true                     # run eval + save best checkpoints
    every: 2_000_000                  # eval cadence in training env steps
    episodes: 10                     # target completed episodes
    min_steps: 5_000                # minimum total steps collected
    max_steps_per_episode: 500_000      # cap per episode (eval-only)
    deterministic: true               # SB3 eval uses deterministic actions
    seed_offset: 10_000               # offset from run.seed for eval seeds
    mode: vectorized                  # vectorized | workers
    n_envs: 10                        # eval env count
#    workers: 1                        # parallel workers (workers mode)

learn:
  total_timesteps: 800_000_000        # total env steps for RL training
  max_steps_per_episode: 5_000          # cap per training episode (env truncation)

algo:
  type: maskable_ppo                  # ppo | maskable_ppo
  params:
    learning_rate: 9.5e-5             # optimizer learning rate
    n_steps: 4096                     # rollout length per environment
    batch_size: 512                   # minibatch size for updates
    n_epochs: 5                       # number of optimization epochs per rollout
    gamma: 0.997                      # discount factor
    ent_coef: 0.01                    # entropy regularization coefficient
#    target_kl: 0.02                   # KL divergence target (optional)
#    clip_range: 0.1                   # PPO clip range (optional)
