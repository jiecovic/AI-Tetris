# @package _group_
policy_kwargs:
  net_arch: [512]                              # options: [] | [512] | [256,256] | {pi:[...], vf:[...]}
  activation_fn: relu                          # options: gelu | gelu_tanh | relu | silu | tanh | identity


feature_extractor:

  spatial_preprocessor:
    type: binary                               # options: binary
    params: {}                                 # binary: no params

  stem:
    type: cnn                                   # options: cnn | conv3x3_32_32_64
    params:
      layers:                                   # options: list[{out,k,s,p,act,pool}]
        - { out: 16, k: 3, s: 1, p: 0, act: relu }  # out>0 k>0 s>0 p>=0 act=gelu|gelu_tanh|relu|silu
        - { out: 32, k: 3, s: 1, p: 0, act: relu }  # out>0 k>0 s>0 p>=0 act=gelu|gelu_tanh|relu|silu
        - { out: 64, k: 3, s: 1, p: 0, act: relu }  # out>0 k>0 s>0 p>=0 act=gelu|gelu_tanh|relu|silu
        - { out: 128, k: 3, s: 1, p: 0, act: relu }
        - { out: 128, k: 1, s: 1, p: 0, act: relu, pool: { type: avg, k: 2, s: 2, p: 0 } }  # pool.type=avg|max; pool k/s>0 p>=0

      # Global defaults applied to every conv layer.
      use_batchnorm: false                      # options: true | false
      dropout: 0.0                              # options: float >= 0

  encoder:
    type: spatial                              # token | spatial

    spatial_head:
      type: flatten_mlp                       # options: global_pool | flatten | flatten_mlp | col_collapse | attn_pool
      params:
        features_dim: auto                      # options: int>0 | auto; auto resolves from head+shape (flatten => H*W*C)
        hidden_dims: [512]                 # options: list[int>0]
        activation: gelu                         # options: gelu | gelu_tanh | relu | silu
        dropout: 0.0                             # options: float >= 0

  feature_augmenter:
    type: mlp_joint                                 # options: none | null | onehot_concat | mlp_joint | mlp_split
    params:
      use_active: true
      use_next: true
      out_dim_total: 64
      out_dim_active: null      # optional, default None
      out_dim_next: null        # optional, default None
      hidden_dims: [64]
      activation: gelu          # gelu | gelu_tanh | relu | silu
      dropout: 0.0
