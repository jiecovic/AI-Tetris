# @package _group_
policy_kwargs:
  net_arch: []
  activation_fn: GELU                          # or whatever mapping you use


feature_extractor:

  spatial_preprocessor:
    type: binary                               # options: binary
    params: {}                                 # binary: no params

  stem:
    type: conv3x3_32_32_64                     # options: cnn | conv3x3_32_32_64 | conv1x3_32x4_64_5l | conv3x3_32_32_64_64_128_5l | conv3x3_32_32_64_row1_col2_128 | conv3x3_32_32_64_row1_col3_128
#    params: {}                                # preset-defined architecture

    # ------------------------------------------------------------------
    # if type: cnn (generic configurable CNN stem)
    # ------------------------------------------------------------------
    # params:
    #   channels: [32, 32, 64]                 # options: list[int], out_channels per conv layer
    #   kernel_sizes: [3, 3, 3]                # options: list[int], odd values only
    #   strides: [1, 1, 1]                     # options: list[int] (default all 1s)
    #   activation: relu                       # options: gelu | relu | silu
    #   dropout: 0.0                           # options: float >= 0
    #   use_batchnorm: false                   # options: true | false (NOT RL-friendly)

  encoder:
    type: spatial                              # token | spatial

    spatial_head:
      type: col_collapse                       # options: global_pool | flatten | flatten_mlp | col_collapse | attn_pool
      params:
        features_dim: 512                      # base encoder output dim (B, F_base)
        collapse: linear
        pool: avgmax                           # options: avg | max | avgmax (if applicable)
        use_batchnorm: false
        include_active_onehot: true
        include_next_onehot: true
        dropout: 0.0

  feature_augmenter:
    type: null                                 # options: none | null | onehot_concat | mlp_joint | mlp_split
#    params:
#      use_active: true                        # include one-hot of active piece kind
#      use_next: true                          # include one-hot(s) of next queue
