# @package _group_
policy_kwargs:
  net_arch: [512]                              # options: [] | [512] | [256,256] | {pi:[...], vf:[...]}
  activation_fn: relu                          # options: gelu | gelu_tanh | relu | silu | tanh | identity; aliases: gelu_none|gelu_approx|gelu_fast|swish|none


feature_extractor:

  spatial_preprocessor:
    type: binary                               # options: binary
    params: {}                                 # binary: no params

  stem:
    type: cnn                                  # options: cnn | conv3x3_32_32_64
    params:
      layers:                                  # options: list[{out,k,s,p,pre_pad,act,pool}]
        - { out: 32,  k: 3,      s: 1,      p: 1,      act: relu }
        - { out: 64,  k: 3,      s: 1,      p: 1,      act: relu }
        - { out: 64,  k: 3,      s: 1,      p: 1,      act: relu }
        - { out: 128, k: [4, 1], s: [2, 1], p: [1, 0], act: relu }  # H: 20->10, W: 10->10
        - { out: 128, k: [4, 1], s: [2, 1], p: [1, 0], act: relu }  # H: 10->5,  W: 10->10
        - { out: 256, k: [5, 1], s: 1,      p: 0,      act: relu }  # H: 5->1,   W: 10->10
        - { out: 256, k: [1, 3], s: 1,      p: [0, 1], act: relu }
        - { out: 256, k: [1, 3], s: 1,      p: [0, 1], act: relu }

      # Global defaults applied to every conv layer.
      use_batchnorm: false                      # options: true | false
      dropout: 0.0                              # options: float >= 0

  encoder:
    type: spatial                              # token | spatial

    spatial_head:
      type: attn_pool
      params:
        features_dim: auto      # int > 0 | auto (auto = n_queries * C_after_stem)
        n_queries: 1            # int > 0
        mlp_hidden: 0           # int >= 0 (0 = no post-MLP)
        activation: gelu        # gelu | gelu_tanh | relu | silu (aliases: gelu_none|gelu_approx|gelu_fast|swish)
        dropout: 0.0            # float >= 0

  feature_augmenter:
    type: mlp_joint                             # options: none | null | onehot_concat | mlp_joint | mlp_split
    params:
      use_active: true
      use_next: true
      out_dim: 32               # options (mlp_joint): int >= 0
      hidden_dims: [32]
      activation: relu          # options: gelu | gelu_tanh | relu | silu
      dropout: 0.0
