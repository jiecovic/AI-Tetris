model:

  policy_kwargs:
    net_arch: [512, 256]
    activation_fn: GELU   # or whatever mapping you use


  feature_extractor:


    spatial_preprocessor:
      type: binary                 # options: binary (later RGB)
      params: {}                   # binary: no params

    stem:
      type: conv3x3_32_32_64_row1_col2_128                     # options: none | cnn | conv3x3_32_32_64
#      params:   # none: no stem applied
#        channels: [32, 64, 64, 128, 256]
#        kernel_sizes: [3, 3, 3, 3, 3]
#        strides: [1, 1, 1, 1, 1]
##        padding: same
#        activation: relu
#        dropout: 0.0
#        use_batchnorm: false

      # ------------------------------------------------------------------
      # if type: conv3x3_32_32_64 (fixed preset, no tuning knobs)
      # ------------------------------------------------------------------
      # params: {}                   # preset-defined architecture

      # ------------------------------------------------------------------
      # if type: cnn (generic configurable CNN stem)
      # ------------------------------------------------------------------
      # params:
      #   channels: [32, 32, 64]     # options: list[int], out_channels per conv layer
      #   kernel_size: 3             # options: int > 0 (usually 3)
      #   stride: 1                  # options: int > 0
      #   padding: same              # options: valid | same | tetris
      #   activation: relu           # options: relu | gelu | identity
      #   dropout: 0.0               # options: float >= 0
      #   use_batchnorm: false       # options: true | false (NOT RL-friendly)


    encoder:
      type: spatial  # token | spatial

      # ------------------------------------------------------------------
      # token encoder (used when type: token_encoder)
      # ------------------------------------------------------------------

      spatial_head:
        features_dim: 128          # base encoder output dim (B, F_base)
        type: global_pool        # options: global_pool | flatten | col_collapse | attn_pool
        params:
#          collapse: linear
          pool: max          # options: mean | max | meanmax (if applicable)
#          include_active_onehot: true
#          include_next_onehot: true

    feature_augmenter:
      type: mlp_joint               # options: null | onehot_concat | mlp_joint | mlp_split
      params:
        use_active: true        # include one-hot of active piece kind
        use_next: true          # include one-hot(s) of next queue
#