# @package _global_
checkpoints:
  latest_every: 20_000                # save latest checkpoint every N env steps

eval:
  mode: both                          # off | rl | imitation | both
  steps: 100_000                      # evaluation step budget (env steps)
  eval_every: 5_000_000               # run eval every N training env steps
  deterministic: true                 # use deterministic policy during eval
  seed_offset: 10_000                 # seed offset added to training base seed

imitation:
  enabled: false                      # enable imitation (behavior cloning) phase
  dataset_dir: datasets/bc/codemy2_uniform_noise
  epochs: 10                          # number of epochs over dataset
  batch_size: 512                     # batch size for BC updates
  learning_rate: 1.0e-4               # optimizer learning rate
  max_grad_norm: 1.0                  # gradient clipping norm
  shuffle: true                       # shuffle dataset each epoch
  max_samples: 0                      # max samples to use (0 = all)
  save_archive: true                  # archive final imitation checkpoint
  archive_dir: checkpoints/imitation  # directory for BC archives

learn:
  total_timesteps: 800_000_000        # total env steps for RL training
#  resume: experiments/cnn_run_013

algo:
  type: maskable_ppo                  # ppo | maskable_ppo
  params:
    learning_rate: 1.0e-4             # optimizer learning rate
    n_steps: 8192                     # rollout length per environment
    batch_size: 512                   # minibatch size for updates
    n_epochs: 2                       # number of optimization epochs per rollout
    gamma: 0.994                      # discount factor
    ent_coef: 0.03                    # entropy regularization coefficient
#    target_kl: 0.02                   # KL divergence target (optional)
#    clip_range: 0.1                   # PPO clip range (optional)
